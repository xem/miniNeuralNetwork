<script>

// Create matrix
// - filled with random numbers by default
// - filled with zero if z = 1
M=(N,o,l)=>Array(N).fill().map(()=>Array(o).fill(l?0:Math.random()));

// Operations on matrices
// o == 0: addition (a+=b)
// o == 1: subtraction (a-=b)
// o == 2: product (a*=b)
// o == 3: scale (a*=b, b is a scalar)
// o == 4: map (a=b(a), b is a function
// o == 5: transpose (r = a^T)
// o == 6: dot product (r = a.b)
O = (a, b, o, r, i, j, k, l = "length") => {
  if(o == 5) r = M(a[0][l], a[l], 1);
  if(o == 6) r = M(a[l], b[0][l], 1);
  for(i = 0; i < a[l]; i++){
    for(j = 0; j < a[0][l]; j++){
      if(o == 0) a[i][j] += b[i][j];
      if(o == 1) a[i][j] -= b[i][j];
      if(o == 2) a[i][j] *= b[i][j];
      if(o == 3) a[i][j] *= b;
      if(o == 4) a[i][j] = b(a[i][j]);
      if(o == 5) r[j][i] = a[i][j];
      if(o == 6){
        for(k = 0; k < b[0][l]; k++){
          r[i][k] += a[i][j] * b[j][k];
        }
      }
    }
  }
  return r;
}

// Mini Neural Network
N = {

  // Init
  
  // Params:
  // - i: input nodes
  // - h: hidden nodes
  // - o: output nodes
  // - l: learning rate
  // - f: activation function (default: sigmoid)
  // - g: gradient descent function (default: y*(1-y))
  
  i: (i, h, o, l = 0.1, f = (x => 1 / (1 + Math.exp(-x))), g = (y => y * (1 - y))) => {
    N.l = l;
    N.f = f;
    N.g = g;

    // Generate random weights from input to hidden, and from hidden to output
    N.w = M(h, i); 
    N.W = M(o, h);

    // Generate random bias for hidden and output
    N.b = M(h, 1);
    N.B = M(o, 1);
  },

  // Passthrough function
  // Used for both training and querying the network
  
  // Params: 
  // - input: input nodes
  // - target: desired output (for training) / undefined (for querying)
  p: (input, target, i, h, o) => {
    
    // Generate hidden nodes's outputs
    i = input;
    h = O(N.w, i, 6);
    O(h, N.b, 0);
    
    // activation function
    O(h, N.f, 4);

    // Generating output nodes's output
    o = O(N.W, h, 6);
    O(o, N.B, 0);
    O(o, N.f, 4);
    
    // Query code ends here
    if(!target) return o;

    // Training coninues here:
    // Calculate the output nodes error (target - output)
    O(target, o, 1);

    // Calculate output nodes gradient
    O(o, N.g, 4);
    O(o, target, 2)
    O(o, N.l, 3);

    // Adjust hidden to output weights with deltas (output gradient x transposed hidden nodes)
    O(N.W, O(o, O(h, 0, 5), 6), 0);
    
    // Adjust bias with gradients
    O(N.B, o, 0); 

    // Calculate hidden gradient, apply the hidden layer errors (Td hidden to output weights x output nodes errors)
    O(h, N.g, 4);
    O(h, O(O(N.W, 0, 5), target, 6), 2);
    O(h, N.l, 3);

    // Adjust input to hidden weights with deltas (hidden gradient x transposed input)
    O(N.w, O(h, O(i, 0, 5), 6), 0);
    
    // Adjust the bias with gradients
    O(N.b, h, 0);
  }
}

// Columnize an array ([a,b] => [[a],[b]])
C = a => a.map(z=>[z]);

N.i(2,3,1);

console.log("Before training...");
console.log('0 AND 0',N.p(C([0,0]))[0][0]);
console.log('0 AND 1',N.p(C([0,1]))[0][0]);
console.log('1 AND 0',N.p(C([1,0]))[0][0]);
console.log('1 AND 1',N.p(C([1,1]))[0][0]);

training_data = [
  [[1,1], [1]],
  [[1,0], [0]],
  [[0,1], [0]],
  [[0,0], [0]],
];

// several training epochs
for(i = 0; i < 10000; i++){
  tdata = training_data[Math.floor(Math.random() * training_data.length)];
  N.p(C(tdata[0]), C(tdata[1]));
}

console.log("After training...");
console.log('0 AND 0',N.p(C([0,0]))[0][0]);
console.log('0 AND 1',N.p(C([0,1]))[0][0]);
console.log('1 AND 0',N.p(C([1,0]))[0][0]);
console.log('1 AND 1',N.p(C([1,1]))[0][0]);


</script>