<script>

// Create matrix
// - filled with random numbers by default
// - filled with zero if z = 1
M = (r,c,z) => Array(r).fill().map(() => Array(c).fill(z ? 0 : Math.random() * 2 - 1));

// Operations on matrices
// o === 0: addition (a+=b)
// o === 1: subtraction (a-=b)
// o === 2: product (a*=b)
// o === 3: scale (a*=b, b is a scalar)
// o === 4: map (a=b(a), b is a function
// b === 0: transpose (r = a^T)
// o === 6: dot product (r = a.b)
O = (a, b, o, r, i, j, k, l = "length") => {
  if(b === 0) r = M(a[0][l], a[l], 1);
  if(o === 6) r = M(a[l], b[0][l], 1);
  for(i = a[l]; i--;){
    for(j = a[0][l]; j--;){
      if(o === 0) a[i][j] += b[i][j];
      if(o === 1) a[i][j] -= b[i][j];
      if(o === 2) a[i][j] *= b[i][j];
      if(o === 3) a[i][j] *= b;
      if(o === 4) a[i][j] = b(a[i][j]);
      if(b === 0) r[j][i] = a[i][j];
      if(o === 6){
        for(k = b[0][l]; k--;){
          r[i][k] += a[i][j] * b[j][k];
        }
      }
    }
  }
  return r;
}

// Mini Neural Network
N = {

  // Init
  
  // Params:
  // - i: input nodes
  // - h: hidden nodes
  // - o: output nodes
  // - l: learning rate
  // - f: activation function (default: sigmoid)
  // - g: gradient descent function (default: y*(1-y))
  
  i: (i, h, o, l = 0.3, f = (x => 1 / (1 + Math.exp(-x))), g = (y => y * (1 - y))) => {
    
    // Save l, f, g for later
    N.l = l;
    N.f = f;
    N.g = g;

    // Generate random weights from input nodes to hidden nodes, and from hidden nodes to output nodes
    N.w = M(h, i); 
    N.W = M(o, h);

    // Generate random bias for hidden and output nodes
    N.b = M(h, 1);
    N.B = M(o, 1);
  },

  // Passthrough function
  // Used for both training and querying the network
  
  // Params: 
  // - input: input nodes
  // - target: desired output (for training) / undefined (for querying)
  p: (input, target, h, o) => {
    
    // Generate hidden nodes' outputs
    O(h = O(N.w, input, 6), N.b, 0);
    
    // Activation function
    O(h, N.f, 4);

    // Generating output nodes' output
    O(o = O(N.W, h, 6), N.B, 0);
    O(o, N.f, 4);
    
    // Query code ends here:
    if(!target) return o;

    // Training coninues here:
    // Calculate output nodes' error (target - output)
    O(target, o, 1);

    // Calculate output nodes' gradient
    O(o, N.g, 4);
    O(o, target, 2)
    O(o, N.l, 3);

    // Adjust hidden to output weights with deltas (output gradient x transposed hidden nodes)
    O(N.W, O(o, O(h, 0), 6), 0);
    
    // Adjust bias with gradients
    O(N.B, o, 0); 

    // Calculate hidden gradient, apply the hidden layer errors (transposed hidden to output weights x output nodes errors)
    O(h, N.g, 4);
    O(h, O(O(N.W, 0), target, 6), 2);
    O(h, N.l, 3);

    // Adjust input to hidden weights with deltas (hidden gradient x transposed input)
    O(N.w, O(h, O(input, 0), 6), 0);
    
    // Adjust the bias with gradients
    O(N.b, h, 0);
  }
}

// Columnize an array ([a,b] => [[a],[b]])
C = a => a.map(z=>[z]);

// Demo
N.i(2,3,1);

console.log("Before training...");
console.log('0 AND 0',N.p(C([0,0]))[0][0]);
console.log('0 AND 1',N.p(C([0,1]))[0][0]);
console.log('1 AND 0',N.p(C([1,0]))[0][0]);
console.log('1 AND 1',N.p(C([1,1]))[0][0]);

training_data = [
  [[1,1], [1]],
  [[1,0], [0]],
  [[0,1], [0]],
  [[0,0], [0]],
];

// several training epochs
for(i = 0; i < 50000; i++){
  tdata = training_data[Math.floor(Math.random() * training_data.length)];
  N.p(C(tdata[0]), C(tdata[1]));
}

console.log("After training...");
console.log('0 AND 0',N.p(C([0,0]))[0][0]);
console.log('0 AND 1',N.p(C([0,1]))[0][0]);
console.log('1 AND 0',N.p(C([1,0]))[0][0]);
console.log('1 AND 1',N.p(C([1,1]))[0][0]);


</script>